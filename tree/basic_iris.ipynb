{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1, 3.5, 1.4, 0.2, 0], [4.9, 3.0, 1.4, 0.2, 0], [4.7, 3.2, 1.3, 0.2, 0], [4.6, 3.1, 1.5, 0.2, 0], [5.0, 3.6, 1.4, 0.2, 0], [5.4, 3.9, 1.7, 0.4, 0], [4.6, 3.4, 1.4, 0.3, 0], [5.0, 3.4, 1.5, 0.2, 0], [4.4, 2.9, 1.4, 0.2, 0], [4.9, 3.1, 1.5, 0.1, 0], [5.4, 3.7, 1.5, 0.2, 0], [4.8, 3.4, 1.6, 0.2, 0], [4.8, 3.0, 1.4, 0.1, 0], [4.3, 3.0, 1.1, 0.1, 0], [5.8, 4.0, 1.2, 0.2, 0], [5.7, 4.4, 1.5, 0.4, 0], [5.4, 3.9, 1.3, 0.4, 0], [5.1, 3.5, 1.4, 0.3, 0], [5.7, 3.8, 1.7, 0.3, 0], [5.1, 3.8, 1.5, 0.3, 0], [5.4, 3.4, 1.7, 0.2, 0], [5.1, 3.7, 1.5, 0.4, 0], [4.6, 3.6, 1.0, 0.2, 0], [5.1, 3.3, 1.7, 0.5, 0], [4.8, 3.4, 1.9, 0.2, 0], [5.0, 3.0, 1.6, 0.2, 0], [5.0, 3.4, 1.6, 0.4, 0], [5.2, 3.5, 1.5, 0.2, 0], [5.2, 3.4, 1.4, 0.2, 0], [4.7, 3.2, 1.6, 0.2, 0], [4.8, 3.1, 1.6, 0.2, 0], [5.4, 3.4, 1.5, 0.4, 0], [5.2, 4.1, 1.5, 0.1, 0], [5.5, 4.2, 1.4, 0.2, 0], [4.9, 3.1, 1.5, 0.1, 0], [5.0, 3.2, 1.2, 0.2, 0], [5.5, 3.5, 1.3, 0.2, 0], [4.9, 3.1, 1.5, 0.1, 0], [4.4, 3.0, 1.3, 0.2, 0], [5.1, 3.4, 1.5, 0.2, 0], [5.0, 3.5, 1.3, 0.3, 0], [4.5, 2.3, 1.3, 0.3, 0], [4.4, 3.2, 1.3, 0.2, 0], [5.0, 3.5, 1.6, 0.6, 0], [5.1, 3.8, 1.9, 0.4, 0], [4.8, 3.0, 1.4, 0.3, 0], [5.1, 3.8, 1.6, 0.2, 0], [4.6, 3.2, 1.4, 0.2, 0], [5.3, 3.7, 1.5, 0.2, 0], [5.0, 3.3, 1.4, 0.2, 0], [7.0, 3.2, 4.7, 1.4, 1], [6.4, 3.2, 4.5, 1.5, 1], [6.9, 3.1, 4.9, 1.5, 1], [5.5, 2.3, 4.0, 1.3, 1], [6.5, 2.8, 4.6, 1.5, 1], [5.7, 2.8, 4.5, 1.3, 1], [6.3, 3.3, 4.7, 1.6, 1], [4.9, 2.4, 3.3, 1.0, 1], [6.6, 2.9, 4.6, 1.3, 1], [5.2, 2.7, 3.9, 1.4, 1], [5.0, 2.0, 3.5, 1.0, 1], [5.9, 3.0, 4.2, 1.5, 1], [6.0, 2.2, 4.0, 1.0, 1], [6.1, 2.9, 4.7, 1.4, 1], [5.6, 2.9, 3.6, 1.3, 1], [6.7, 3.1, 4.4, 1.4, 1], [5.6, 3.0, 4.5, 1.5, 1], [5.8, 2.7, 4.1, 1.0, 1], [6.2, 2.2, 4.5, 1.5, 1], [5.6, 2.5, 3.9, 1.1, 1], [5.9, 3.2, 4.8, 1.8, 1], [6.1, 2.8, 4.0, 1.3, 1], [6.3, 2.5, 4.9, 1.5, 1], [6.1, 2.8, 4.7, 1.2, 1], [6.4, 2.9, 4.3, 1.3, 1], [6.6, 3.0, 4.4, 1.4, 1], [6.8, 2.8, 4.8, 1.4, 1], [6.7, 3.0, 5.0, 1.7, 1], [6.0, 2.9, 4.5, 1.5, 1], [5.7, 2.6, 3.5, 1.0, 1], [5.5, 2.4, 3.8, 1.1, 1], [5.5, 2.4, 3.7, 1.0, 1], [5.8, 2.7, 3.9, 1.2, 1], [6.0, 2.7, 5.1, 1.6, 1], [5.4, 3.0, 4.5, 1.5, 1], [6.0, 3.4, 4.5, 1.6, 1], [6.7, 3.1, 4.7, 1.5, 1], [6.3, 2.3, 4.4, 1.3, 1], [5.6, 3.0, 4.1, 1.3, 1], [5.5, 2.5, 4.0, 1.3, 1], [5.5, 2.6, 4.4, 1.2, 1], [6.1, 3.0, 4.6, 1.4, 1], [5.8, 2.6, 4.0, 1.2, 1], [5.0, 2.3, 3.3, 1.0, 1], [5.6, 2.7, 4.2, 1.3, 1], [5.7, 3.0, 4.2, 1.2, 1], [5.7, 2.9, 4.2, 1.3, 1], [6.2, 2.9, 4.3, 1.3, 1], [5.1, 2.5, 3.0, 1.1, 1], [5.7, 2.8, 4.1, 1.3, 1], [6.3, 3.3, 6.0, 2.5, 2], [5.8, 2.7, 5.1, 1.9, 2], [7.1, 3.0, 5.9, 2.1, 2], [6.3, 2.9, 5.6, 1.8, 2], [6.5, 3.0, 5.8, 2.2, 2], [7.6, 3.0, 6.6, 2.1, 2], [4.9, 2.5, 4.5, 1.7, 2], [7.3, 2.9, 6.3, 1.8, 2], [6.7, 2.5, 5.8, 1.8, 2], [7.2, 3.6, 6.1, 2.5, 2], [6.5, 3.2, 5.1, 2.0, 2], [6.4, 2.7, 5.3, 1.9, 2], [6.8, 3.0, 5.5, 2.1, 2], [5.7, 2.5, 5.0, 2.0, 2], [5.8, 2.8, 5.1, 2.4, 2], [6.4, 3.2, 5.3, 2.3, 2], [6.5, 3.0, 5.5, 1.8, 2], [7.7, 3.8, 6.7, 2.2, 2], [7.7, 2.6, 6.9, 2.3, 2], [6.0, 2.2, 5.0, 1.5, 2], [6.9, 3.2, 5.7, 2.3, 2], [5.6, 2.8, 4.9, 2.0, 2], [7.7, 2.8, 6.7, 2.0, 2], [6.3, 2.7, 4.9, 1.8, 2], [6.7, 3.3, 5.7, 2.1, 2], [7.2, 3.2, 6.0, 1.8, 2], [6.2, 2.8, 4.8, 1.8, 2], [6.1, 3.0, 4.9, 1.8, 2], [6.4, 2.8, 5.6, 2.1, 2], [7.2, 3.0, 5.8, 1.6, 2], [7.4, 2.8, 6.1, 1.9, 2], [7.9, 3.8, 6.4, 2.0, 2], [6.4, 2.8, 5.6, 2.2, 2], [6.3, 2.8, 5.1, 1.5, 2], [6.1, 2.6, 5.6, 1.4, 2], [7.7, 3.0, 6.1, 2.3, 2], [6.3, 3.4, 5.6, 2.4, 2], [6.4, 3.1, 5.5, 1.8, 2], [6.0, 3.0, 4.8, 1.8, 2], [6.9, 3.1, 5.4, 2.1, 2], [6.7, 3.1, 5.6, 2.4, 2], [6.9, 3.1, 5.1, 2.3, 2], [5.8, 2.7, 5.1, 1.9, 2], [6.8, 3.2, 5.9, 2.3, 2], [6.7, 3.3, 5.7, 2.5, 2], [6.7, 3.0, 5.2, 2.3, 2], [6.3, 2.5, 5.0, 1.9, 2], [6.5, 3.0, 5.2, 2.0, 2], [6.2, 3.4, 5.4, 2.3, 2], [5.9, 3.0, 5.1, 1.8, 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "#print(iris_data.data)\n",
    "#print(iris_data.target)\n",
    "iris_data = datasets.load_iris()\n",
    "dataSet=[]\n",
    "for i,example in enumerate(iris_data.data):\n",
    "    data = list(example)\n",
    "    data.append(iris_data.target[i])\n",
    "    #print(data)\n",
    "    dataSet.append(data)\n",
    "print(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "def createDataSet():\n",
    "    iris_data = datasets.load_iris()\n",
    "    dataSet=[]\n",
    "    for i,example in enumerate(iris_data.data):\n",
    "        data = list(example)\n",
    "        data.append(iris_data.target[i])\n",
    "        #print(data)\n",
    "        dataSet.append(data)\n",
    "    labels=list(set(iris_data.target))\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet) #nrows\n",
    "    #为所有的分类类目创建字典\n",
    "    labelCounts ={}\n",
    "    for featVec in dataSet:\n",
    "        currentLable=featVec[-1] #取得最后一列数据\n",
    "        if currentLable not in labelCounts.keys():\n",
    "            labelCounts[currentLable]=0\n",
    "        labelCounts[currentLable]+=1\n",
    "    #计算香农熵\n",
    "    shannonEnt=0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义按照某个特征进行划分的函数splitDataSet\n",
    "#输入三个变量（待划分的数据集，特征，分类值）\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    retDataSet=[]\n",
    "    retDataSet1=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]<=value : \n",
    "            reduceFeatVec=featVec[:axis]\n",
    "            reduceFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reduceFeatVec)\n",
    "        else: \n",
    "            reduceFeatVec=featVec[:axis]\n",
    "            reduceFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet1.append(reduceFeatVec)\n",
    "    return retDataSet,retDataSet1 #返回不含划分特征的子集\n",
    "\n",
    "#定义按照最大信息增益划分数据的函数\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeature=len(dataSet[0])-1\n",
    "    baseEntropy=calcShannonEnt(dataSet)#香农熵\n",
    "    bestInforGain=0\n",
    "    bestFeature=-1\n",
    "    bestValue=-1\n",
    "    for i in range(numFeature):\n",
    "        featList=[number[i] for number in dataSet] #得到某个特征下所有值（某列）\n",
    "        uniqualVals=set(featList) #set无重复的属性特征值\n",
    "        newEntropy=0\n",
    "        for value in uniqualVals:\n",
    "            subDataSet,subDataSet1=splitDataSet(dataSet,i,value)\n",
    "            prob=len(subDataSet)/float(len(dataSet)) #即p(t)\n",
    "            prob1=len(subDataSet1)/float(len(dataSet))\n",
    "            newEntropy=prob*calcShannonEnt(subDataSet)+prob1*calcShannonEnt(subDataSet1)#对各子集香农熵求和\n",
    "            infoGain=baseEntropy-newEntropy #计算信息增益\n",
    "        #最大信息增益\n",
    "            if (infoGain>bestInforGain):\n",
    "                bestInforGain=infoGain\n",
    "                bestFeature=i\n",
    "                bestValue=value\n",
    "    return bestFeature,bestValue #返回特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "#投票表决代码\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount=sorted(classCount.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    #类别相同，停止划分\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    #长度为1，返回出现次数最多的类别\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    #按照信息增益最高选取分类特征属性\n",
    "    bestFeat,bestVal=chooseBestFeatureToSplit(dataSet)#返回分类的特征序号\n",
    "    bestFeatLable=labels[bestFeat] #该特征的label\n",
    "    myTree={bestFeatLable:{}} #构建树的字典\n",
    "    del(labels[bestFeat]) #从labels的list中删除该label\n",
    "    subLables=labels[:] #子集合\n",
    "        #构建数据的子集合，并进行递归\n",
    "    myTree[bestFeatLable]['<='+str(bestVal)]=createTree(splitDataSet(dataSet,bestFeat,bestVal)[0],subLables)\n",
    "    subLables=labels[:] \n",
    "    myTree[bestFeatLable]['>'+str(bestVal)]=createTree(splitDataSet(dataSet,bestFeat,bestVal)[1],subLables)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#输入三个变量（决策树，属性特征标签，测试的数据）\n",
    "def classify(inputTree,featLables,testVec):\n",
    "    firstStr=list(inputTree.keys())[0] #获取树的第一个特征属性\n",
    "    secondDict=inputTree[firstStr] #树的分支，子集合Dict\n",
    "    featIndex=featLables.index(firstStr) #获取决策树第一层在featLables中的位置\n",
    "    for key in secondDict.keys():\n",
    "        if '<=' in key:\n",
    "            val = float(key[2:])\n",
    "            if testVec[featIndex]<=val:\n",
    "                if type(secondDict[key]).__name__=='dict':\n",
    "                    classLabel=classify(secondDict[key],featLables,testVec)\n",
    "                else:classLabel=secondDict[key]\n",
    "        else:\n",
    "            val = float(key[1:])\n",
    "            if testVec[featIndex]>val:\n",
    "                if type(secondDict[key]).__name__=='dict':\n",
    "                    classLabel=classify(secondDict[key],featLables,testVec)\n",
    "                else:classLabel=secondDict[key]\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setalLength': {'>1.9': {'setalLength': {'>1.7': {'spealLength': {'<=5.9': {'spealwidth': {'>3.0': 1, '<=3.0': 2}}, '>5.9': 2}}, '<=1.7': {'spealLength': {'<=7.0': {'spealwidth': {'<=2.8': 1, '>2.8': 1}}, '>7.0': 2}}}}, '<=1.9': 0}}\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "myDat, labels = createDataSet()\n",
    "data_train, data_test= \\\n",
    "    train_test_split(myDat, test_size = 0.2, random_state = 42)\n",
    "myTree = createTree(data_train,['spealLength', 'spealwidth', 'setalLength', 'setalLength']) \n",
    "print(myTree)\n",
    "feaList = ['spealLength', 'spealwidth', 'setalLength', 'setalLength']\n",
    "count = 0\n",
    "for testData in data_test:\n",
    "    dic = classify(myTree, feaList, testData[:-1])\n",
    "    if dic == testData[-1]:\n",
    "        count+=1\n",
    "accuracy = count/len(data_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
